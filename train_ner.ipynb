{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e00995b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import utils_data as ut\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from transformers import BertForMaskedLM, BertTokenizer\n",
    "tokenizerBERT = BertTokenizer.from_pretrained('pranav-s/MaterialsBERT', model_max_length=512)\n",
    "modelBERT = BertForMaskedLM.from_pretrained('pranav-s/MaterialsBERT')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6413d04d",
   "metadata": {},
   "outputs": [],
   "source": [
    "classes = {'POLYMER': 1,\n",
    "           'ORGANIC': 2,\n",
    "           'MONOMER': 3,\n",
    "           'PROP_NAME': 4,\n",
    "           'INORGANIC': 5,\n",
    "           'MATERIAL_AMOUNT': 6,\n",
    "           'POLYMER_FAMILY': 7,\n",
    "           'PROP_VALUE': 8,\n",
    "           'O': 0}\n",
    "max_length = 512\n",
    "batch_size = 3\n",
    "class NERBERTModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.bert = modelBERT.base_model\n",
    "        self.linear = nn.Linear(768, len(classes) + 1)\n",
    "        \n",
    "    def forward(self, token):\n",
    "        encoder_output= self.bert(token)  # torch.LongTensor of shape (batch_size, sequence_length)\n",
    "        linear_output = self.linear(encoder_output.last_hidden_state)\n",
    "        class_output = F.softmax(linear_output, dim=2)\n",
    "        return class_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "784af378",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = NERBERTModel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bb25e7db",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_data = 100\n",
    "data_list = ut.read_data('train.json', max_length)[:num_data]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fe8eceb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "token_tensors_all_list = [ut.list2token(tokenizerBERT, d['words'], max_length) for d in data_list]\n",
    "data = torch.cat(token_tensors_all_list, dim=0)\n",
    "data_batches = ut.to_batches(data, batch_size)\n",
    "target_tensors_all_list = [ut.cat2digit(classes, d['ner'], max_length) for d in data_list]\n",
    "target = torch.stack(target_tensors_all_list, dim=0)\n",
    "target_batches = ut.to_batches(target, batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "94aff78a",
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.0002)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "366bff7e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "We strongly recommend passing in an `attention_mask` since your input_ids may be padded. See https://huggingface.co/docs/transformers/troubleshooting#incorrect-output-when-padding-tokens-arent-masked.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:  0  batch:    0  loss: 2.34516931\n",
      "epoch:  0  batch:    1  loss: 2.07965207\n",
      "epoch:  0  batch:    2  loss: 1.89227831\n",
      "epoch:  0  batch:    3  loss: 1.86859322\n",
      "epoch:  0  batch:    4  loss: 1.74767876\n",
      "epoch:  0  batch:    5  loss: 1.58365214\n",
      "epoch:  0  batch:    6  loss: 1.53962743\n",
      "epoch:  0  batch:    7  loss: 1.52590048\n",
      "epoch:  0  batch:    8  loss: 1.53084755\n",
      "epoch:  0  batch:    9  loss: 1.52945483\n",
      "epoch:  0  batch:   10  loss: 1.53164637\n",
      "epoch:  0  batch:   11  loss: 1.55527401\n",
      "epoch:  0  batch:   12  loss: 1.53272164\n",
      "epoch:  0  batch:   13  loss: 1.52031934\n",
      "epoch:  0  batch:   14  loss: 1.50896406\n",
      "epoch:  0  batch:   15  loss: 1.51920211\n",
      "epoch:  0  batch:   16  loss: 1.53089154\n",
      "epoch:  0  batch:   17  loss: 1.52350187\n",
      "epoch:  0  batch:   18  loss: 1.51018870\n",
      "epoch:  0  batch:   19  loss: 1.50567687\n",
      "epoch:  0  batch:   20  loss: 1.53294098\n",
      "epoch:  0  batch:   21  loss: 1.52948940\n",
      "epoch:  0  batch:   22  loss: 1.52433884\n",
      "epoch:  0  batch:   23  loss: 1.53214121\n",
      "epoch:  0  batch:   24  loss: 1.54837167\n",
      "epoch:  0  batch:   25  loss: 1.55289090\n",
      "epoch:  0  batch:   26  loss: 1.51106918\n",
      "epoch:  0  batch:   27  loss: 1.48127127\n",
      "epoch:  0  batch:   28  loss: 1.53591931\n",
      "epoch:  0  batch:   29  loss: 1.51894367\n",
      "epoch:  0  batch:   30  loss: 1.53123653\n",
      "epoch:  0  batch:   31  loss: 1.52213192\n",
      "epoch:  0  batch:   32  loss: 1.50520027\n",
      "epoch:  0  batch:   33  loss: 1.49667358\n",
      "epoch:  1  batch:    0  loss: 1.53894222\n",
      "epoch:  1  batch:    1  loss: 1.51271927\n",
      "epoch:  1  batch:    2  loss: 1.51653230\n",
      "epoch:  1  batch:    3  loss: 1.53202307\n",
      "epoch:  1  batch:    4  loss: 1.52709615\n",
      "epoch:  1  batch:    5  loss: 1.51891649\n",
      "epoch:  1  batch:    6  loss: 1.50070441\n",
      "epoch:  1  batch:    7  loss: 1.51240063\n",
      "epoch:  1  batch:    8  loss: 1.52151346\n",
      "epoch:  1  batch:    9  loss: 1.52346313\n",
      "epoch:  1  batch:   10  loss: 1.52698934\n",
      "epoch:  1  batch:   11  loss: 1.54234219\n",
      "epoch:  1  batch:   12  loss: 1.53062022\n",
      "epoch:  1  batch:   13  loss: 1.51936305\n",
      "epoch:  1  batch:   14  loss: 1.50585926\n",
      "epoch:  1  batch:   15  loss: 1.51689732\n",
      "epoch:  1  batch:   16  loss: 1.52795112\n",
      "epoch:  1  batch:   17  loss: 1.52144289\n",
      "epoch:  1  batch:   18  loss: 1.69161093\n",
      "epoch:  1  batch:   19  loss: 1.50529802\n",
      "epoch:  1  batch:   20  loss: 1.53339541\n",
      "epoch:  1  batch:   21  loss: 1.53054464\n",
      "epoch:  1  batch:   22  loss: 1.52504873\n",
      "epoch:  1  batch:   23  loss: 1.53327501\n",
      "epoch:  1  batch:   24  loss: 1.55005395\n",
      "epoch:  1  batch:   25  loss: 1.55495501\n",
      "epoch:  1  batch:   26  loss: 1.51049030\n",
      "epoch:  1  batch:   27  loss: 1.48043954\n",
      "epoch:  1  batch:   28  loss: 1.53428257\n",
      "epoch:  1  batch:   29  loss: 1.51796436\n",
      "epoch:  1  batch:   30  loss: 1.53026497\n",
      "epoch:  1  batch:   31  loss: 1.52038491\n",
      "epoch:  1  batch:   32  loss: 1.50408554\n",
      "epoch:  1  batch:   33  loss: 1.49501002\n"
     ]
    }
   ],
   "source": [
    "epochs = 2\n",
    "train_losses = []\n",
    "for epoch in range(epochs):\n",
    "    for b, X in enumerate(data_batches):\n",
    "        y_pred = model(X)\n",
    "        y_pred = torch.swapaxes(y_pred, 1, 2)\n",
    "        y = target_batches[b]\n",
    "        \n",
    "        loss = criterion(y_pred, y)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        print(f'epoch: {epoch:2}  batch: {b:4}  loss: {loss.item():10.8f}')\n",
    "    train_losses.append(loss)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "8a1b0be8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([9.9898e-01, 1.2063e-04, 8.4833e-05, 7.8012e-05, 8.3950e-05, 1.2519e-04,\n",
       "        7.3147e-05, 1.1248e-04, 8.3863e-05, 2.5312e-04],\n",
       "       grad_fn=<SelectBackward0>)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred[0, :, 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "12914f8d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 5, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0,\n",
       "         0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 6, 6, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 6, 6, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9,\n",
       "         9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9,\n",
       "         9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9,\n",
       "         9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9,\n",
       "         9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9,\n",
       "         9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9,\n",
       "         9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9,\n",
       "         9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9,\n",
       "         9, 9, 9, 9, 9, 9, 9, 9]])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a82da196",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([100, 512])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70c3492d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "ae7bf36f5dbf0881f49ddb9294144751d972892b8d789f369fe04d31bdb1fea1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
